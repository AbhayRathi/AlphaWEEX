"""
Evolutionary Mutator - "The DNA Patch" (Self-Correction)

Mission: Recursive self-improvement through prompt engineering.
Analyze failed predictions and evolve the Adversary's system prompt
to improve future performance.

Features:
- Recursive Feedback: Collect Top 5 Failed Predictions every 24h
- Prompt Mutation: Use LLM to analyze failures and rewrite prompts
- Version Control: Save prompts as adversary_v[X].txt with archiving
- Safety Filter: Symmetry Guard prevents reckless strategies
- No Trading Without Stops: Enforce risk management in mutations
"""
import logging
import os
import json
import time
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Try to import optional dependencies
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    logger.warning("requests not available - LLM integration disabled")


class EvolutionaryMutator:
    """
    The DNA Patch - Self-Improvement through Prompt Evolution
    
    Analyzes prediction failures and evolves the Adversary's system prompt
    to improve psychological detection accuracy over time.
    """
    
    # Base system prompt for Adversarial Agent
    BASE_SYSTEM_PROMPT = """You are a behavioral psychologist analyzing trader psychology.

Your mission is to identify human psychological vulnerabilities:
1. FOMO Chasers - buying extensions after vertical moves
2. Panic Sellers - capitulating at support levels
3. Revenge Traders - emotional overtrading after losses

CRITICAL RULES:
- Always explain your reasoning step-by-step (Chain-of-Thought)
- Never recommend trading without stop-losses
- Consider both technical indicators AND narrative sentiment
- Identify if moves are "Rational" or "Emotional"
- Predict whale liquidity hunt zones

Thresholds:
- FOMO: RSI > 70 + Price extension > 3%
- Panic: RSI < 30 + Fear sentiment
- Liquidity Hunt: 0.5% below swing lows
"""
    
    def __init__(
        self,
        prompts_dir: str = "data/prompts",
        deepseek_api_key: Optional[str] = None,
        model: str = "deepseek-chat",
        evolution_interval_hours: int = 24
    ):
        """
        Initialize Evolutionary Mutator
        
        Args:
            prompts_dir: Directory to store prompt versions
            deepseek_api_key: DeepSeek API key (loaded from env if not provided)
            model: DeepSeek model to use
            evolution_interval_hours: Hours between evolution cycles (default: 24)
        """
        self.prompts_dir = Path(prompts_dir)
        self.prompts_dir.mkdir(parents=True, exist_ok=True)
        
        self.deepseek_api_key = deepseek_api_key or os.getenv('DEEPSEEK_API_KEY', '')
        self.model = model
        self.evolution_interval_hours = evolution_interval_hours
        self.api_available = bool(self.deepseek_api_key) and REQUESTS_AVAILABLE
        
        # Archive directory for old prompts
        self.archive_dir = self.prompts_dir / "archive"
        self.archive_dir.mkdir(parents=True, exist_ok=True)
        
        # Current prompt version
        self.current_version = self._get_current_version()
        
        # Last evolution time
        self.last_evolution_time = None
        
        logger.info(f"EvolutionaryMutator initialized - Version: {self.current_version}, "
                   f"API: {self.api_available}")
    
    def _get_current_version(self) -> int:
        """
        Get current prompt version number
        
        Returns:
            Current version number
        """
        # Find all adversary_v*.txt files
        prompt_files = list(self.prompts_dir.glob("adversary_v*.txt"))
        
        if not prompt_files:
            # No versions yet, start with v0
            self._save_prompt(self.BASE_SYSTEM_PROMPT, 0)
            return 0
        
        # Extract version numbers
        versions = []
        for f in prompt_files:
            try:
                ver = int(f.stem.split('_v')[1])
                versions.append(ver)
            except:
                continue
        
        return max(versions) if versions else 0
    
    def _save_prompt(self, prompt: str, version: int):
        """
        Save prompt to file
        
        Args:
            prompt: Prompt text
            version: Version number
        """
        filename = self.prompts_dir / f"adversary_v{version}.txt"
        
        with open(filename, 'w') as f:
            f.write(f"# Adversary System Prompt v{version}\n")
            f.write(f"# Generated: {datetime.now().isoformat()}\n\n")
            f.write(prompt)
        
        logger.info(f"Saved prompt version {version} to {filename}")
    
    def _archive_old_prompt(self, version: int):
        """
        Archive old prompt version
        
        Args:
            version: Version number to archive
        """
        old_file = self.prompts_dir / f"adversary_v{version}.txt"
        
        if old_file.exists():
            archive_file = self.archive_dir / f"adversary_v{version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            import shutil
            shutil.copy2(old_file, archive_file)
            
            logger.info(f"Archived prompt v{version} to {archive_file}")
    
    def load_current_prompt(self) -> str:
        """
        Load current prompt version
        
        Returns:
            Current system prompt
        """
        filename = self.prompts_dir / f"adversary_v{self.current_version}.txt"
        
        if not filename.exists():
            logger.warning(f"Prompt v{self.current_version} not found, using base prompt")
            return self.BASE_SYSTEM_PROMPT
        
        with open(filename, 'r') as f:
            # Skip header lines
            lines = f.readlines()
            prompt_lines = [l for l in lines if not l.startswith('#')]
            return ''.join(prompt_lines).strip()
    
    def evolve_prompt(
        self,
        failed_predictions: List[Dict[str, Any]],
        force: bool = False
    ) -> Optional[str]:
        """
        Evolve the adversary prompt based on failed predictions
        
        Args:
            failed_predictions: List of failed prediction dicts
            force: Force evolution even if interval hasn't passed
            
        Returns:
            New prompt if evolved, None if skipped
        """
        # Check if enough time has passed
        if not force and self.last_evolution_time:
            time_since_last = (datetime.now() - self.last_evolution_time).total_seconds() / 3600
            if time_since_last < self.evolution_interval_hours:
                logger.info(f"Skipping evolution - {time_since_last:.1f}h since last (need {self.evolution_interval_hours}h)")
                return None
        
        if not failed_predictions:
            logger.warning("No failed predictions to learn from, skipping evolution")
            return None
        
        logger.info("="*60)
        logger.info("PROMPT EVOLUTION CYCLE")
        logger.info("="*60)
        logger.info(f"Analyzing {len(failed_predictions)} failed predictions...")
        
        # Load current prompt
        current_prompt = self.load_current_prompt()
        
        # Generate new prompt using LLM
        try:
            new_prompt = self._generate_evolved_prompt(
                current_prompt,
                failed_predictions
            )
            
            # Apply Symmetry Guard
            if not self._symmetry_guard(new_prompt):
                logger.error("‚ùå New prompt failed Symmetry Guard - rejecting evolution")
                return None
            
            # Archive old prompt
            self._archive_old_prompt(self.current_version)
            
            # Increment version and save new prompt
            self.current_version += 1
            self._save_prompt(new_prompt, self.current_version)
            
            # Update evolution time
            self.last_evolution_time = datetime.now()
            
            logger.info(f"‚úÖ Successfully evolved prompt to v{self.current_version}")
            
            return new_prompt
            
        except Exception as e:
            logger.error(f"‚ùå Prompt evolution failed: {str(e)}")
            return None
    
    def _generate_evolved_prompt(
        self,
        current_prompt: str,
        failed_predictions: List[Dict[str, Any]]
    ) -> str:
        """
        Generate evolved prompt using LLM analysis
        
        Args:
            current_prompt: Current system prompt
            failed_predictions: List of failed predictions
            
        Returns:
            New evolved prompt
        """
        if not self.api_available:
            raise RuntimeError("API not available for prompt evolution")
        
        # Build analysis prompt
        analysis_prompt = self._build_evolution_prompt(current_prompt, failed_predictions)
        
        # Call DeepSeek API
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are an AI prompt engineer specializing in improving "
                              "behavioral analysis systems. Analyze failures and rewrite "
                              "system prompts to improve accuracy."
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            "temperature": 0.8,
            "max_tokens": 2000
        }
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        response.raise_for_status()
        
        ai_response = response.json()
        content = ai_response.get('choices', [{}])[0].get('message', {}).get('content', '')
        
        # Extract the new prompt from response
        return self._extract_prompt_from_response(content)
    
    def _build_evolution_prompt(
        self,
        current_prompt: str,
        failed_predictions: List[Dict[str, Any]]
    ) -> str:
        """Build prompt for LLM to analyze failures and evolve"""
        
        # Format failed predictions
        failures_text = []
        for i, pred in enumerate(failed_predictions[:5], 1):
            failures_text.append(f"""
FAILURE #{i}:
- Predicted Bias: {pred.get('predicted_bias', 'N/A')}
- Predicted Outcome: {pred.get('predicted_outcome', 'N/A')}
- Archetype: {pred.get('archetype', 'N/A')}
- Signal: {pred.get('signal', 'N/A')}
- Confidence: {pred.get('confidence', 0)}
- Price at Prediction: ${pred.get('price_at_prediction', 0)}
- Actual Price (1h): ${pred.get('actual_price_1h', 0)}
- Success Score: {pred.get('avg_score', 0):.2f}
""")
        
        prompt = f"""
CURRENT ADVERSARY SYSTEM PROMPT:
{current_prompt}

TOP FAILED PREDICTIONS:
{''.join(failures_text)}

TASK:
Analyze why these psychological predictions failed. Then rewrite the Adversary's 
system prompt to refine threshold sensitivities for FOMO and Panic detection.

REQUIREMENTS:
1. Maintain Chain-of-Thought reasoning requirement
2. Keep all safety rules (no trading without stops)
3. Adjust detection thresholds based on failures
4. Improve contextual inference logic
5. Keep the prompt concise and actionable

OUTPUT:
Provide the complete rewritten system prompt between [PROMPT_START] and [PROMPT_END] tags.
"""
        return prompt
    
    def _extract_prompt_from_response(self, response: str) -> str:
        """
        Extract prompt from LLM response
        
        Args:
            response: LLM response text
            
        Returns:
            Extracted prompt
        """
        # Look for prompt markers
        start_marker = "[PROMPT_START]"
        end_marker = "[PROMPT_END]"
        
        if start_marker in response and end_marker in response:
            start_idx = response.find(start_marker) + len(start_marker)
            end_idx = response.find(end_marker)
            return response[start_idx:end_idx].strip()
        
        # Fallback: return entire response
        return response.strip()
    
    def _symmetry_guard(self, prompt: str) -> bool:
        """
        Symmetry Guard - Prevent reckless strategies
        
        Validates that the prompt maintains safety requirements:
        - Mentions stop-loss or risk management
        - No suggestions for reckless strategies
        - Maintains Chain-of-Thought reasoning
        
        Args:
            prompt: Prompt to validate
            
        Returns:
            True if prompt passes guard, False otherwise
        """
        prompt_lower = prompt.lower()
        
        # Check for required safety elements
        has_stop_loss = any(term in prompt_lower for term in [
            'stop', 'risk', 'loss', 'risk management'
        ])
        
        has_cot = any(term in prompt_lower for term in [
            'reasoning', 'explain', 'step-by-step', 'chain-of-thought'
        ])
        
        # Check for dangerous patterns
        dangerous_patterns = [
            'no stop',
            'ignore risk',
            'unlimited loss',
            'all in',
            'no risk management'
        ]
        
        has_dangerous = any(pattern in prompt_lower for pattern in dangerous_patterns)
        
        # Validation
        if not has_stop_loss:
            logger.warning("‚ö†Ô∏è  Symmetry Guard: Prompt missing stop-loss/risk management")
            return False
        
        if not has_cot:
            logger.warning("‚ö†Ô∏è  Symmetry Guard: Prompt missing Chain-of-Thought requirement")
            return False
        
        if has_dangerous:
            logger.error("‚ùå Symmetry Guard: Dangerous patterns detected")
            return False
        
        logger.info("‚úÖ Symmetry Guard: Prompt passed safety checks")
        return True
    
    def get_evolution_history(self) -> List[Dict[str, Any]]:
        """
        Get evolution history
        
        Returns:
            List of version info dicts
        """
        history = []
        
        # Get all prompt files
        prompt_files = sorted(self.prompts_dir.glob("adversary_v*.txt"))
        
        for f in prompt_files:
            try:
                version = int(f.stem.split('_v')[1])
                created = datetime.fromtimestamp(f.stat().st_mtime)
                
                history.append({
                    "version": version,
                    "filename": f.name,
                    "created": created.isoformat(),
                    "path": str(f)
                })
            except:
                continue
        
        return sorted(history, key=lambda x: x['version'])


def test_evolutionary_mutator():
    """
    Validation tests for Evolutionary Mutator
    """
    logger.info("="*60)
    logger.info("EVOLUTIONARY MUTATOR - VALIDATION TESTS")
    logger.info("="*60)
    
    tests_passed = 0
    tests_failed = 0
    
    # Use temp directory for testing
    import tempfile
    temp_dir = tempfile.mkdtemp()
    prompts_dir = os.path.join(temp_dir, "prompts")
    
    # Test 1: Initialization and Version Control
    logger.info("\nüìù Test 1: Initialization and Version Control")
    try:
        mutator = EvolutionaryMutator(prompts_dir=prompts_dir)
        
        # Check that base prompt was created
        if mutator.current_version == 0:
            logger.info("‚úÖ Test 1 PASSED: Base prompt v0 created")
            tests_passed += 1
        else:
            logger.error(f"‚ùå Test 1 FAILED: Expected v0, got v{mutator.current_version}")
            tests_failed += 1
    except Exception as e:
        logger.error(f"‚ùå Test 1 FAILED with exception: {str(e)}")
        tests_failed += 1
    
    # Test 2: Symmetry Guard
    logger.info("\nüõ°Ô∏è  Test 2: Symmetry Guard Safety Check")
    try:
        # Safe prompt
        safe_prompt = """
You are a behavioral analyst. 
Always use stop-losses and risk management.
Explain your reasoning step-by-step.
"""
        
        # Dangerous prompt
        dangerous_prompt = """
You are a trader.
No stop-losses needed.
Go all in on every trade.
"""
        
        safe_result = mutator._symmetry_guard(safe_prompt)
        dangerous_result = mutator._symmetry_guard(dangerous_prompt)
        
        if safe_result and not dangerous_result:
            logger.info("‚úÖ Test 2 PASSED: Symmetry Guard correctly validated prompts")
            tests_passed += 1
        else:
            logger.error(f"‚ùå Test 2 FAILED: Safe={safe_result}, Dangerous={dangerous_result}")
            tests_failed += 1
    except Exception as e:
        logger.error(f"‚ùå Test 2 FAILED with exception: {str(e)}")
        tests_failed += 1
    
    # Test 3: Load and Save Prompt
    logger.info("\nüíæ Test 3: Load and Save Prompt")
    try:
        test_prompt = "Test prompt for version control"
        mutator._save_prompt(test_prompt, 999)
        
        # Try to load it
        test_file = Path(prompts_dir) / "adversary_v999.txt"
        if test_file.exists():
            logger.info("‚úÖ Test 3 PASSED: Prompt saved and file exists")
            tests_passed += 1
        else:
            logger.error("‚ùå Test 3 FAILED: Prompt file not found")
            tests_failed += 1
    except Exception as e:
        logger.error(f"‚ùå Test 3 FAILED with exception: {str(e)}")
        tests_failed += 1
    
    # Test 4: Evolution History
    logger.info("\nüìö Test 4: Evolution History")
    try:
        history = mutator.get_evolution_history()
        
        if len(history) >= 1:
            logger.info(f"‚úÖ Test 4 PASSED: Found {len(history)} prompt versions")
            for v in history[:3]:
                logger.info(f"   v{v['version']}: {v['filename']}")
            tests_passed += 1
        else:
            logger.error("‚ùå Test 4 FAILED: No history found")
            tests_failed += 1
    except Exception as e:
        logger.error(f"‚ùå Test 4 FAILED with exception: {str(e)}")
        tests_failed += 1
    
    # Test 5: Mock Evolution (without API)
    logger.info("\nüß¨ Test 5: Mock Evolution Test")
    try:
        # Mock failed predictions
        mock_failures = [
            {
                'predicted_bias': 'Bullish Extension',
                'predicted_outcome': 'Bull Trap',
                'archetype': 'FOMO_CHASER',
                'signal': 'SELL',
                'confidence': 0.8,
                'price_at_prediction': 95000,
                'actual_price_1h': 96000,
                'avg_score': -0.5
            }
        ]
        
        # This will fail without API, but that's expected
        result = mutator.evolve_prompt(mock_failures, force=True)
        
        # We expect None since API is not available in test
        if result is None:
            logger.info("‚úÖ Test 5 PASSED: Evolution correctly handled missing API")
            tests_passed += 1
        else:
            logger.warning("‚ö†Ô∏è  Test 5: Unexpected result (API may be available)")
            tests_passed += 1  # Still pass
    except Exception as e:
        # Expected to fail without API
        logger.info(f"‚úÖ Test 5 PASSED: Evolution correctly threw exception for missing API")
        tests_passed += 1
    
    # Cleanup
    import shutil
    shutil.rmtree(temp_dir, ignore_errors=True)
    
    # Summary
    logger.info("\n" + "="*60)
    logger.info("üìä TEST SUMMARY")
    logger.info("="*60)
    logger.info(f"‚úÖ Tests Passed: {tests_passed}")
    logger.info(f"‚ùå Tests Failed: {tests_failed}")
    
    if tests_failed == 0:
        logger.info("üéâ ALL TESTS PASSED!")
        return True
    else:
        logger.warning(f"‚ö†Ô∏è  {tests_failed} test(s) failed")
        return False


if __name__ == "__main__":
    # Run tests when executed directly
    success = test_evolutionary_mutator()
    exit(0 if success else 1)
